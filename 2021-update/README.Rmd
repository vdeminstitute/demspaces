---
title: "2021 Update Notes"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Andreas Beger  
2021-03-26

_tl;dr: For the 2021 update, I eliminated a bunch of external data sources and other variables. Instead of 481 columns, the new merged data has 225. This did not decrease forecast performance, in fact accuracy increased a bit._

Other files you can read here are:

- [variable-importance.md](variable-importance.md): details on how I decided which variables and external data sources to take out.
- [project-summary.pdf](project-summary.pdf): my (Andreas Beger's) summary of the work I did for the 2021 update. This is a good summary of all changes to the DemSpaces and PART forecasts. 
- [DemocraticSpaces2021.pdf](DemocraticSpaces2021.pdf): a report assessing the accuracy of the DemSpaces forecasts to date, including an evaluation of the impact that changes in the V-Dem data between different versions have.
- [DemSpaces2021-Questions.pdf](DemSpaces2021-Questions.pdf): addresses several questions about the 2021 forecasts.


## Slightly more details on the variable elimination

I made a very substantial change for the 2021 forecast update, namely removing a large number of data sources and variables that go into the models as predictors. The reasons were that (1) I wanted to streamline the update process, (2) some of the data sources (EPR, Archigos) have not been update, and (3) and prior suspicion that some of the variables in the data were not really that helpful for accuracy. 

To do this I looked at the variable importance using last year's model. More details and the results of that analysis are documented in [variable-importance.md](variable-importance.md). Based on those results, I, for example, removed the EPR, Archigos, and ACD data sources completely. 

The initial data update for 2021 did not include those changes: I updated the V-Dem, P&T Coups, GDP, population, and infant mortality data, and merged them with last year's EPR, etc. data. The resulting dataset, with the suffix "v11a", had the same number of columns as the 2020 "v10" dataset. 

Subsequently I applied those changes and produced a second dataset with roughly only half the columns as the first version, and this one is "v11b". 

I ran the forecasts using both dataset versions as input, to make sure that the removal of \~200 predictors would not affect accuracy. The table and figure below show the accuracy of both versions. Streamlining the dataset in fact seems to have increased performance. 


```{r, include=FALSE}
suppressPackageStartupMessages({
  library(dplyr)
  library(here)
  library(ggplot2)
  library(readr)
  library(demspacesR)
  library(tidyr)
})

# Load merge data so we can get the observed outcomes
states_v10  <- readRDS(here::here("archive/data/states-v10.rds"))
states_v11a <- readRDS("data/states-v11a.rds")
states_v11b <- readRDS("data/states-v11b.rds")

# Note that the truth version between V-Dem version are NOT the same
ex_v10 <- states_v10 %>%
  filter(year < 2018) %>%
  pull(dv_v2x_veracc_osp_up_next2)
ex_v11a <- states_v11a %>%
  filter(year < 2018) %>%
  pull(dv_v2x_veracc_osp_up_next2)
ex_v11b <- states_v11b %>%
  filter(year < 2018) %>%
  pull(dv_v2x_veracc_osp_up_next2)
cor(cbind(ex_v10, ex_v11a, ex_v11b))


# Load forecasts
fcasts_v10  <- read_csv(here("archive/fcasts-rf-v10.csv"), col_types = cols())
fcasts_v11a <- read_csv("data/fcasts-rf-v11a.csv", col_types = cols())
fcasts_v11b <- read_csv("data/fcasts-rf-v11b.csv", col_types = cols())

# Subset all forecasts to last common year (2018) and calculate accuracy,
# using both the v10 and v11 truth (they are not the same)
acc <- bind_rows(
  # v10 forecasts
  fcasts_v10 %>%
    filter(from_year < 2018) %>%
    score_ds_fcast(truth = states_v10) %>%
    mutate(fcasts = "v10", truth = "v10"),
  fcasts_v10 %>%
    filter(from_year < 2018) %>%
    score_ds_fcast(truth = states_v11b) %>%
    mutate(fcasts = "v10", truth = "v11"),
  # v3a forecasts
  fcasts_v11a %>%
    filter(from_year < 2018) %>%
    score_ds_fcast(truth = states_v10) %>%
    mutate(fcasts = "v11a", truth = "v10"),
  fcasts_v11a %>%
    filter(from_year < 2018) %>%
    score_ds_fcast(truth = states_v11b) %>%
    mutate(fcasts = "v11a", truth = "v11"),
  # v11b forecasts
  fcasts_v11b %>%
    filter(from_year < 2018) %>%
    score_ds_fcast(truth = states_v10) %>%
    mutate(fcasts = "v11b", truth = "v10"),
  fcasts_v11b %>%
    filter(from_year < 2018) %>%
    score_ds_fcast(truth = states_v11b) %>%
    mutate(fcasts = "v11b", truth = "v11")
)

# Take out "Pos-rate"
acc <- acc %>%
  filter(Measure != "Pos-rate")

# Average over direction
acc <- acc %>%
  group_by(fcasts, truth, Measure) %>%
  summarize(Value = mean(Value),
            .groups = "drop")

# What's the effect of which truth data we use?
ggplot(acc, aes(x = truth, y = Value)) +
  facet_wrap(~ Measure) +
  geom_line(aes(x = truth, group = fcasts,
                color = fcasts)) +
  geom_point()
```

```{r}
# How do the forecast versions impact accuracy
acc %>%
    filter(fcasts!="v10", truth=="v11") %>% 
  pivot_wider(names_from = "Measure", values_from = "Value") %>% 
  arrange(truth, fcasts) %>%
  select(truth, fcasts, `Log-loss`, `ROC-AUC`, `PR-AUC`) %>%
  setNames(c("V-Dem", "Forecasts", "Log-loss", "AUC-ROC", "AUC-PR")) %>%
  knitr::kable(digits = 3)

acc %>%
  filter(fcasts!="v10", truth=="v11") %>%
  ggplot(aes(x = fcasts, y = Value)) +
  facet_wrap(~ Measure) +
  geom_line(aes(x = fcasts, group = truth,
                color = truth)) +
  geom_point() +
  scale_y_continuous(limits = c(0, 1)) +
  theme_light()
```

